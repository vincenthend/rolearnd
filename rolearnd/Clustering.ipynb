{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritma Clustering\n",
    "\n",
    "- 13515089 - Vincent Hendryanto Halim\n",
    "- 13515099 - Mikhael Artur Darmakesuma\n",
    "- 13515107 - Roland Hartanto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deskripsi Singkat\n",
    "\n",
    "### 1.1 K-Means\n",
    "K-Means mencari centroid yang akan digunakan sebagai titik tengah suatu cluster. Data akan dipasangkan pada cluster berdasarkan kedekatan pada cluster tersebut. Setelah memasangkan data pada cluster, titik tengah pada cluster akan dihitung ulang untuk melakukan pemasangan ulang data pada titik tengah cluster yang baru. Algoritma akan berhenti bila cluster tidak berpindah lagi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "// K-Means pseudocode\n",
    "initialize_centroid()\n",
    "cluster_new = pair_data_with_centroid()\n",
    "do    \n",
    "    cluster_old = cluster_new\n",
    "    find_new_centroid()\n",
    "    cluster_new = pair_data_with_centroid()    \n",
    "while(cluster_new != cluster_old)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 K-Medoids\n",
    "K-Medoids menggunakan medoid berdasarkan data yang dimiliki. Data akan dipasangkan pada cluster berdasarkan kedekatan dengan titik medoids. Setelah memasangkan data pada cluster, titik medoid akan dicoba dipindah menggunakan data lain pada cluster. Setelah itu akan dihitung nilai error dengan menggunakan rumus absolute error dan dibandingan dengan nilai error pada cluster saat ini\n",
    "\n",
    "$ E = \\sum_{j=1}^{k}\\sum_{p\\in C_j}\\left|p-o_j\\right| $\n",
    "\n",
    "dengan $k$ adalah jumlah cluster, $p$ adalah data pada cluster, dan $o_j$ adalah medoid pada cluster\n",
    "\n",
    "Jika didapatkan nilai error menurun, maka medoid akan berpindah ke data yang baru saja diubah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```js\n",
    "// K-Medoids pseudocode\n",
    "initialize_medoid()\n",
    "cluster_new = pair_data_with_medoid()\n",
    "do    \n",
    "    cluster_old = cluster_new\n",
    "    old_error = count_error(cluster_old)\n",
    "    for each (swappable_pair_of_medoid)\n",
    "        cluster_swap = swap_cluster()\n",
    "        swap_error = count_error(cluster_swap)\n",
    "        if(old_error > swap_error)\n",
    "            cluster_new = cluster_swap\n",
    "while(cluster_new != cluster_old)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Agglomerative\n",
    "Algoritma agglomerative memasangkan data berdasarkan kedekatan data dengan data yang lain.\n",
    "\n",
    "Untuk memasangkan data pertama dihitung matriks jarak untuk N data. Kemudian dicari jarak minimum antara dua data untuk memasangkan antara dua data\n",
    "\n",
    "$D(C_i,C_j)= \\min_{1\\leq m,l\\lt N,m\\neq l}  D(C_m, C_l)$\n",
    "\n",
    "Matriks jarak kemudian diupdate untuk membandingkan jarak antara cluster dengan data yang lain. Proses terus diulang hingga membentuk jumlah cluster yang diinginkan atau satu cluster besar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```js\n",
    "// Agglomerative pseudocode\n",
    "dist_matrix = count_distance_matrix()\n",
    "for [n_data..n_cluster]\n",
    "    a,b = find_closest_pair(dist_matrix)\n",
    "    dist_matrix = update_dist_matrix(dist_matrix)    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 DBSCAN\n",
    "DBSCAN memasangkan titik berdasarkan dua nilai, yaitu $\\epsilon$ dan $minpts$. Titik core adalah titik yang memiliki tetangga dengan kurang dari $\\epsilon$ lebih dari atau sama dengan nilai $minpts$. Titik yang berhasil dipasangkan dengan titik lain akan menjadi cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```js\n",
    "// DBSCAN pesudocode\n",
    "dist_matrix = count_distance_matrix()\n",
    "for each data\n",
    "    n_neighbors = count neighbors(eps)\n",
    "    if (n_neighbors > min_pts)\n",
    "        give_label_to_data()\n",
    "        give_label_to_neighbors()\n",
    "        adjust_density_reachable_data_label()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Source Code dan Penjelasan Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KMeans(Classifier):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    num_clusters : int, number of centroid/means/cluster\n",
    "    init : {'random', 'np.ndarray or list(user defined)'}\n",
    "           initial centroids\n",
    "    max_iteration : maximum iteration limit\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    means : cluster centers/centroids\n",
    "    labels : labels of each instance\n",
    "    num_clusters : number of cluster\n",
    "    init : initial means\n",
    "    max_iteration : maximum iteration limit\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_clusters=3, init='random', max_iteration=300,  **kwargs):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.init = init\n",
    "        self.max_iteration = max_iteration\n",
    "\n",
    "    def fit(self, X : DataFrame):\n",
    "        '''\n",
    "        '''\n",
    "        num_of_instances = X.shape[0]\n",
    "        \n",
    "        if self.num_clusters > num_of_instances:\n",
    "            raise ValueError(\n",
    "                \"num_of_instances = %d must be larger than num_clusters = %d\" % (num_of_instances, self.num_clusters))\n",
    "        else:\n",
    "            if (self.num_clusters == 1):\n",
    "                self.labels = [0 for i in range(0, num_of_instances)]\n",
    "                self.means = self.update_means(labels, X)\n",
    "            else:\n",
    "                init_type = type(self.init)\n",
    "                if (init_type == str):\n",
    "                    if (self.init == 'random'):\n",
    "                        means = [X.iloc[random.randrange(0, num_of_instances)] for i in range(0, self.num_clusters)]\n",
    "                        self.k_means(means, X)\n",
    "                    else:\n",
    "                        raise ValueError(\"Init type 'random' or ndarray expected, found %s\" % (self.init))\n",
    "                else:\n",
    "                    if (len(self.init) == self.num_clusters):\n",
    "                        self.k_means(self.init, X)\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"number of initial means = %d must be equal to num_clusters = %d\" % (len(self.init), self.num_clusters))\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = self.count_distances(self.means, X)\n",
    "        return self.assign_labels(distances)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        y = 0 # dummy ... may be deleted soon (?)\n",
    "        self.fit(X, y)\n",
    "        return self.labels\n",
    "    \n",
    "    def k_means(self, initial_means, data):\n",
    "        means = initial_means\n",
    "        prev_means = [[0 for j in range(0, len(means[0]))] for i in range(0, len(means))]\n",
    "        iteration = 0\n",
    "        while ((not self.is_means_equal(means, prev_means)) and (iteration < self.max_iteration)):\n",
    "            distances = self.count_distances(means, data)\n",
    "            # print(means)\n",
    "            labels = self.assign_labels(distances)\n",
    "            # print(labels)\n",
    "            prev_means = means\n",
    "            means = self.update_means(labels, data)\n",
    "            iteration += 1\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.means = means\n",
    "\n",
    "    def count_distances(self, means, data):\n",
    "        num_of_instances = data.shape[0]\n",
    "        distances = [[-1 for j in range(0, len(means))] for i in range(0, num_of_instances)]\n",
    "        #distances = [[-1 for j in range(0, num_of_instances)] for i in range(0, len(means))]\n",
    "        \n",
    "        for instance_idx in range(0, num_of_instances):\n",
    "            instance_data = [x for x in data.iloc[instance_idx]]\n",
    "            # print(instance_data)\n",
    "            for means_idx in range(0, len(means)):\n",
    "                distances[instance_idx][means_idx] = self.calculate_euclidean_dist(means[means_idx], instance_data)\n",
    "        \n",
    "        return distances\n",
    "\n",
    "    def calculate_euclidean_dist(self, attribute1, attribute2):\n",
    "        squared_distance = 0\n",
    "\n",
    "        if (len(attribute1) == len(attribute2)):\n",
    "            for i in range(0, len(attribute1)):\n",
    "                squared_distance += pow((attribute1[i] - attribute2[i]), 2)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"number of attributes must be equal, attribute1 = %d, attribute2 = %d\" % (len(attribute1), len(attribute2)))\n",
    "        \n",
    "        return math.sqrt(squared_distance)\n",
    "    \n",
    "    def is_means_equal(self, means, prev_means):\n",
    "        is_equal = True\n",
    "        num_of_attributes = len(means[0])\n",
    "\n",
    "        for i in range(0, len(means)):\n",
    "            for j in range(0, num_of_attributes):\n",
    "                if(not (means[i][j] == prev_means[i][j])):\n",
    "                    is_equal = False\n",
    "                    break\n",
    "            if (not is_equal):\n",
    "                break\n",
    "        \n",
    "        return is_equal\n",
    "    \n",
    "    def assign_labels(self, distances):\n",
    "        labels = [-1 for i in range(0, len(distances))]\n",
    "        for i in range(0, len(distances)):\n",
    "            idx, val = self.min_val(distances[i])\n",
    "            labels[i] = idx\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "    def min_val(self, list_elements):\n",
    "        index = -1\n",
    "        value = sys.maxsize\n",
    "        for i in range(0, len(list_elements)):\n",
    "            if (list_elements[i] < value):\n",
    "                index = i\n",
    "                value = list_elements[i]\n",
    "\n",
    "        return index, value\n",
    "    \n",
    "    def update_means(self, labels, data):\n",
    "        means = [[0 for j in range(0, data.shape[1])] for i in range(0, self.num_clusters)]\n",
    "        sums = [[0 for j in range(0, data.shape[1])] for i in range(0, self.num_clusters)]\n",
    "        n_cluster_elmt = [0 for i in range(0, self.num_clusters)]\n",
    "\n",
    "        for j in range(0, data.shape[1]): # access by column\n",
    "            for i in range(0, len(labels)):\n",
    "                for k in range(0, self.num_clusters):\n",
    "                    if (k == labels[i]):\n",
    "                        sums[k][j] += data[j][i]\n",
    "                        if (j == 0):\n",
    "                            n_cluster_elmt[k] += 1\n",
    "        \n",
    "        for cluster_idx in range(0, self.num_clusters):\n",
    "            for attr_idx in range(0, data.shape[1]):\n",
    "                means[cluster_idx][attr_idx] = sums[cluster_idx][attr_idx] / n_cluster_elmt[cluster_idx]\n",
    "\n",
    "        return means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 K-Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KMedoids(Classifier):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    num_clusters : int, number of centroid/means/cluster\n",
    "    init : {'random', 'np.ndarray or list(user defined)'}\n",
    "           initial medoids\n",
    "    max_iteration : maximum iteration limit\n",
    "    swap_medoid : {'optimized', 'random'}\n",
    "                  swap medoid method, optimized = search for the minimum possible swap, \n",
    "                  random = random swap\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    medoids : cluster centers\n",
    "    labels : labels of each instance\n",
    "    num_clusters : number of cluster\n",
    "    init : initial means\n",
    "    max_iteration : maximum iteration limit\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_clusters=3, init='random', max_iteration=300, swap_medoid='optimized', **kwargs):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.init = init\n",
    "        self.max_iteration = max_iteration\n",
    "        self.swap_medoid = swap_medoid\n",
    "\n",
    "    def fit(self, X : DataFrame):\n",
    "        self.train_data = X\n",
    "        num_of_instances = X.shape[0]\n",
    "        \n",
    "        if self.num_clusters > num_of_instances:\n",
    "            raise ValueError(\n",
    "                \"num_of_instances = %d must be larger than num_clusters = %d\" % (num_of_instances, self.num_clusters))\n",
    "        else:\n",
    "            if (self.num_clusters == 1):\n",
    "                self.labels = [0 for i in range(0, num_of_instances)]\n",
    "                # self.medoids = self.update_medoids(labels, X)\n",
    "            else:\n",
    "                init_type = type(self.init)\n",
    "                if (init_type == str):\n",
    "                    if (self.init == 'random'):\n",
    "                        medoids = [random.randrange(0, num_of_instances) for i in range(0, self.num_clusters)]\n",
    "                        self.k_medoids(medoids, X)\n",
    "                    else:\n",
    "                        raise ValueError(\"Init type 'random' or ndarray expected, found %s\" % (self.init))\n",
    "                else:\n",
    "                    if (len(self.init) == self.num_clusters):\n",
    "                        # init : list of index of selected initial medoids\n",
    "                        self.k_medoids(self.init, X)\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"number of initial means = %d must be equal to num_clusters = %d\" % (len(self.init), self.num_clusters))\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = self.count_distances(self.medoids, X)\n",
    "        return self.assign_labels(distances)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels\n",
    "    \n",
    "    def k_medoids(self, initial_medoids, data):\n",
    "        medoids = initial_medoids\n",
    "        iteration = 0\n",
    "        while (iteration < self.max_iteration):\n",
    "            # print(medoids)\n",
    "            distances = self.count_distances(medoids, data)\n",
    "            labels = self.assign_labels(distances)\n",
    "            error = self.calculate_error(distances)\n",
    "            # print(error)\n",
    "            \n",
    "            new_distances = []\n",
    "            new_medoids = []\n",
    "            new_error = 0\n",
    "            if (self.swap_medoid == 'optimized'):\n",
    "                # daftar error tiap swap buat tiap instance [swap cluster, error]\n",
    "                swap_errors = [[0, 0] for i in range(0, data.shape[0])]\n",
    "                # swap buat tiap cluster i\n",
    "                for i in range(0, self.num_clusters):\n",
    "                    swap_candidate_idxs = []\n",
    "                    for j in range(0, len(labels)):\n",
    "                        if (labels[j] == i):\n",
    "                            swap_candidate_idxs.append(j)\n",
    "                    \n",
    "                    for j in range(0, len(swap_candidate_idxs)):\n",
    "                        new_medoids = []\n",
    "                        new_medoids = [x for x in medoids]\n",
    "                        new_medoids[i] = swap_candidate_idxs[j]\n",
    "                        new_distances = self.count_distances(new_medoids, data)\n",
    "                        new_error = self.calculate_error(new_distances)\n",
    "                        swap_errors[swap_candidate_idxs[j]] = [i, new_error]\n",
    "                \n",
    "                min_error = sys.maxsize\n",
    "                min_swap_idx = -1\n",
    "                medoid_to_swap = -1\n",
    "                for i in range(0, len(swap_errors)):\n",
    "                    if (swap_errors[i][1] < min_error):\n",
    "                        min_error = swap_errors[i][1]\n",
    "                        min_swap_idx = i\n",
    "                        medoid_to_swap = swap_errors[i][0]\n",
    "                # print(error, min_error)\n",
    "                if(min_error - error < 0):\n",
    "                    medoids[medoid_to_swap] = min_swap_idx\n",
    "                    # print(self.calculate_error(self.count_distances(medoids, data)))\n",
    "                else:\n",
    "                    break\n",
    "            elif (self.swap_medoid == 'random'):\n",
    "                temp_medoid_to_swap = random.randrange(0, self.num_clusters)\n",
    "                swap_candidate_idxs = []\n",
    "                for i in range(0, len(labels)):\n",
    "                    if (labels[i] == temp_medoid_to_swap):\n",
    "                        swap_candidate_idxs.append(i)\n",
    "                new_medoids = medoids\n",
    "                new_medoids[temp_medoid_to_swap] = random.choice(swap_candidate_idxs)\n",
    "                new_distances = self.count_distances(new_medoids, data)\n",
    "                new_error = self.calculate_error(new_distances)\n",
    "                if (new_error - error < 0):\n",
    "                    medoids = new_medoids\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"swap_medoid must be equal to 'optimized' or 'random', found %d\" % (self.swap_medoid))\n",
    "\n",
    "            iteration += 1\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.medoids = medoids\n",
    "\n",
    "    def count_distances(self, medoids, data):\n",
    "        num_of_instances = data.shape[0]\n",
    "        distances = [[-1 for j in range(0, len(medoids))] for i in range(0, num_of_instances)]\n",
    "        \n",
    "        for instance_idx in range(0, num_of_instances):\n",
    "            instance_data = [x for x in data.iloc[instance_idx]]\n",
    "            for medoid_idx in range(0, len(medoids)):\n",
    "                medoid_instance = [x for x in self.train_data.iloc[medoids[medoid_idx]]]\n",
    "                distances[instance_idx][medoid_idx] = self.calculate_absolute_dist(medoid_instance, instance_data)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def calculate_absolute_dist(self, attribute1, attribute2):\n",
    "        absolute_distance = 0\n",
    "\n",
    "        if (len(attribute1) == len(attribute2)):\n",
    "            for i in range(0, len(attribute1)):\n",
    "                absolute_distance += abs(attribute1[i] - attribute2[i])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"number of attributes must be equal, attribute1 = %d, attribute2 = %d\" % (len(attribute1), len(attribute2)))\n",
    "        \n",
    "        return absolute_distance\n",
    "    \n",
    "    def assign_labels(self, distances):\n",
    "        labels = [-1 for i in range(0, len(distances))]\n",
    "        for i in range(0, len(distances)):\n",
    "            idx, val = self.min_val(distances[i])\n",
    "            labels[i] = idx\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def calculate_error(self, distances):\n",
    "        total_error = 0\n",
    "        for i in range(0, len(distances)):\n",
    "            idx, val = self.min_val(distances[i])\n",
    "            total_error += val\n",
    "        \n",
    "        return total_error\n",
    "\n",
    "    def min_val(self, list_elements):\n",
    "        index = -1\n",
    "        value = sys.maxsize\n",
    "        for i in range(0, len(list_elements)):\n",
    "            if (list_elements[i] < value):\n",
    "                index = i\n",
    "                value = list_elements[i]\n",
    "\n",
    "        return index, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agglomerative(Classifier):\n",
    "    '''\n",
    "    Agglomerative Clustering class for clustering\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    link = [\"single\", \"complete\", \"average\", \"average-group\"]\n",
    "        Link type for distance between cluster\n",
    "    distance = [\"manhattan\", \"euclidean\"]\n",
    "        Distance for distance matrix\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    Fit and predict in separate process\n",
    "    >>> classifier = Agglomerative()\n",
    "    >>> classifier.fit(X)\n",
    "    >>> classifier.predict()\n",
    "\n",
    "    Using fit_predict\n",
    "    >>> class\n",
    "    >>> classifier = Agglomerative()\n",
    "    >>> classifier.fit_predict(X)\n",
    "    \n",
    "    Take note that X is a dataframe\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link=\"single\", n_cluster=2, distance=\"manhattan\", **kwargs):\n",
    "        link_type = [\"single\", \"complete\", \"average\", \"average-group\"]\n",
    "        dist_type = [\"euclidean\", \"manhattan\"]\n",
    "        if(link in link_type):\n",
    "            self.link_type = link\n",
    "        else:\n",
    "            raise ValueError(\"Link type %s not supported\" % link)\n",
    "        if(distance in dist_type):\n",
    "            self.distance = distance\n",
    "        else:\n",
    "            raise ValueError(\"Link type %s not supported\" % link)\n",
    "        if(n_cluster > 0):\n",
    "            self.n_cluster = n_cluster\n",
    "        else:\n",
    "            raise ValueError(\"n_cluster must be > 0\")\n",
    "\n",
    "    def fit(self, X : DataFrame):\n",
    "        self.__data = X.copy()\n",
    "        n_row, n_col = X.shape\n",
    "        self.__n_elmt = n_row\n",
    "        \n",
    "        self.__createDistMatrix(self.__data)\n",
    "        for i in range(self.__n_elmt-self.n_cluster):\n",
    "            # Select the smallest distance index\n",
    "            min_idx = np.argmin(self.dist_matrix)\n",
    "            min_row, min_col = self.__idxToRowCol(min_idx, n_row, n_row)\n",
    "            \n",
    "            # Generate new distance matrix\n",
    "            combined_node = self.__generateNode(min_row, min_col)\n",
    "            self.__removeNode(min_row, min_col)\n",
    "            self.__mergeNode(combined_node)\n",
    "\n",
    "    def predict(self):       \n",
    "        flatten_cluster = self.__flattenCluster()\n",
    "\n",
    "        result_array = np.array([-1 for n in range(self.__n_elmt)])\n",
    "        for cluster in range(len(flatten_cluster)):\n",
    "            for i in flatten_cluster[cluster]:\n",
    "                result_array[i] = cluster\n",
    "\n",
    "        return result_array\n",
    "\n",
    "    def fit_predict(self, X : DataFrame):\n",
    "        self.fit(x)\n",
    "        return self.predict() \n",
    "\n",
    "    def __flattenCluster(self):\n",
    "        return [self.__flatten(f) for f in self.cluster_tree]\n",
    "\n",
    "    def __generateNode(self, a, b):\n",
    "        if(self.link_type == \"single\"):\n",
    "            row_a = self.dist_matrix[a,]\n",
    "            row_b = self.dist_matrix[b,]\n",
    "            new_row = np.minimum(row_a, row_b)\n",
    "            new_row = np.delete(new_row, a)\n",
    "            # Handle row shift\n",
    "            if(a<b):\n",
    "                new_row = np.delete(new_row, b-1)\n",
    "            else:\n",
    "                new_row = np.delete(new_row, b)\n",
    "            new_row = np.append(new_row, float('inf'))\n",
    "        elif(self.link_type == \"complete\"):\n",
    "            row_a = self.dist_matrix[a,]\n",
    "            row_b = self.dist_matrix[b,]\n",
    "            new_row = np.maximum(row_a, row_b)\n",
    "            new_row = np.delete(new_row, a)\n",
    "            # Handle row shift\n",
    "            if(a<b):\n",
    "                new_row = np.delete(new_row, b-1)\n",
    "            else:\n",
    "                new_row = np.delete(new_row, b)\n",
    "            new_row = np.append(new_row, float('inf'))\n",
    "        elif(self.link_type == \"average\"):\n",
    "            # Get list of element for cluster and non cluster\n",
    "            cluster_el = [self.cluster_tree[a], self.cluster_tree[b]]\n",
    "            opposite_cluster = []\n",
    "            \n",
    "            for idx in range(len(self.cluster_tree)):\n",
    "                if(idx != a and idx != b):\n",
    "                    opposite_cluster.append(self.cluster_tree[idx])\n",
    "            \n",
    "            cluster_el = self.__flatten(cluster_el)\n",
    "\n",
    "            # Count distance between cluster and other cluster\n",
    "            new_row = np.array([])\n",
    "            for opposite_cluster_el in opposite_cluster:\n",
    "                flat_op_elmt = self.__flatten(opposite_cluster_el)\n",
    "                for op_el in flat_op_elmt:\n",
    "                    sum_dist = 0\n",
    "                    for el in cluster_el:\n",
    "                        sum_dist += self.dist_matrix_orig[el,op_el]\n",
    "                        \n",
    "                new_row = np.append(new_row, [sum_dist/len(cluster_el)])\n",
    "            new_row = np.append(new_row, float('inf'))\n",
    "\n",
    "        elif(self.link_type == \"average-group\"):\n",
    "            # Get list of element for cluster and non cluster\n",
    "            cluster_el = [self.cluster_tree[a], self.cluster_tree[b]]\n",
    "            opposite_cluster = []\n",
    "            \n",
    "            for idx in range(len(self.cluster_tree)):\n",
    "                if(idx != a and idx != b):\n",
    "                    opposite_cluster.append(self.cluster_tree[idx])\n",
    "            \n",
    "            cluster_el = self.__flatten(cluster_el)\n",
    "            # Get cluster center\n",
    "            cluster_center = self.__data.iloc[cluster_el].mean()            \n",
    "\n",
    "            new_row = np.array([])            \n",
    "            # Count distance between cluster center and other cluster center\n",
    "            for opposite_cluster_el in opposite_cluster:\n",
    "                flat_op_elmt = self.__flatten(opposite_cluster_el)\n",
    "                op_cluster_center = self.__data.iloc[flat_op_elmt].mean()\n",
    "                dist = math.fabs(op_cluster_center.subtract(cluster_center,fill_value=0).abs().sum()) \n",
    "                new_row = np.append(new_row, dist)\n",
    "            new_row = np.append(new_row, float('inf'))\n",
    "        return new_row\n",
    "\n",
    "    def __removeNode(self, a, b):\n",
    "        a_val = self.cluster_tree.pop(a)\n",
    "        self.dist_matrix = np.delete(self.dist_matrix,a,0)\n",
    "        self.dist_matrix = np.delete(self.dist_matrix,a,1)\n",
    "        if(a<b):\n",
    "            self.dist_matrix = np.delete(self.dist_matrix,b-1,0)\n",
    "            self.dist_matrix = np.delete(self.dist_matrix,b-1,1)\n",
    "            b_val = self.cluster_tree.pop(b-1)\n",
    "        else:\n",
    "            self.dist_matrix = np.delete(self.dist_matrix,b,0)\n",
    "            self.dist_matrix = np.delete(self.dist_matrix,b,1)\n",
    "            b_val = self.cluster_tree.pop(b)\n",
    "        self.cluster_tree.append([a_val,b_val])\n",
    "\n",
    "    def __mergeNode(self, node):\n",
    "        n_row, n_col = self.dist_matrix.shape        \n",
    "        \n",
    "        b = np.zeros((n_row+1, n_col+1))\n",
    "        b[:-1,:-1] = self.dist_matrix\n",
    "        self.dist_matrix = b\n",
    "\n",
    "        for i in range(n_row+1):\n",
    "            self.dist_matrix[i,n_col] = node[i]\n",
    "            self.dist_matrix[n_col,i] = node[i]\n",
    "\n",
    "    def __createDistMatrix(self, X : DataFrame):\n",
    "        self.dist_matrix = np.ndarray(shape=(self.__n_elmt, self.__n_elmt))\n",
    "        self.cluster_tree = [n for n in range(self.__n_elmt)]\n",
    "        for index, row in X.iterrows():\n",
    "            for index_2, row_2 in X.iterrows():\n",
    "                if(index_2 == index):\n",
    "                    distance = float('inf')\n",
    "                else:\n",
    "                    if(self.distance == \"manhattan\"):\n",
    "                        distance = math.fabs(row_2.subtract(row,fill_value=0).abs().sum())\n",
    "                    elif(self.distance==\"euclidean\"):\n",
    "                        distance = (row_2.subtract(row,fill_value=0).pow(2).sum()) ** 0.5\n",
    "                self.dist_matrix.itemset((index, index_2), distance)\n",
    "        \n",
    "        # Create copy for average - averagegroup\n",
    "        self.dist_matrix_orig = self.dist_matrix.copy()\n",
    "\n",
    "    def __idxToRowCol(self, idx, n_row, n_col):\n",
    "        return (int(idx/n_row),idx%n_col)\n",
    "\n",
    "    def __flatten(self, x):\n",
    "        if isinstance(x, collections.Iterable):\n",
    "            return [a for i in x for a in self.__flatten(i)]\n",
    "        else:\n",
    "            return [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBSCAN(Classifier):\n",
    "    '''\n",
    "    DBSCAN class for clustering\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    eps : float, minimal neighbor distance\n",
    "    min_pts: int, minimal neighbors within eps distance, including itself\n",
    "    distance: {\"euclidean\", \"manhattan\"}, distance measuring algorithm\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    eps : float, minimal neighbor distance\n",
    "    min_pts: int, minimal neighbors within eps distance, including itself\n",
    "    distance: {\"euclidean\", \"manhattan\"}, distance measuring algorithm\n",
    "    __data: DataFrame, data to cluster\n",
    "    __n_elmt: int, number of data\n",
    "    distance_matrix: ndarray, distance matrix\n",
    "    labels : int, labels of each instance\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, eps=0.5, min_pts=5, distance=\"euclidean\", **kwargs):\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "        distance_type = [\"euclidean\", \"manhattan\"]\n",
    "        if (distance in distance_type):\n",
    "            self.distance = distance\n",
    "        else:\n",
    "            raise ValueError(\"Distance type %s not supported\" % distance)\n",
    "\n",
    "    def fit(self, X: DataFrame):\n",
    "        # Get data's information\n",
    "        self.__data = X.copy()\n",
    "        n_row, n_col = X.shape\n",
    "        self.__n_elmt = n_row\n",
    "\n",
    "        # Create distance matrix\n",
    "        self.__createDistanceMatrix(self.__data)\n",
    "\n",
    "        self.labels = [-1 for i in range(self.__n_elmt)]\n",
    "        for i in range (self.__n_elmt):\n",
    "            # Count neighbors with distance <= eps, including itself\n",
    "            n_neighbor = 1 # Count itself\n",
    "            neighbor_labels = []\n",
    "            for j in range (self.__n_elmt):\n",
    "                if (i != j):\n",
    "                    if (self.distance_matrix[i, j] <= self.eps):\n",
    "                        n_neighbor += 1\n",
    "                        # Check for density reachable data's label\n",
    "                        if ((self.labels[j] != -1) and not(self.labels[j] in neighbor_labels)):\n",
    "                            neighbor_labels.append(self.labels[j])\n",
    "\n",
    "            # Data have at leas min_pts neighbors, data is core point\n",
    "            if (n_neighbor >= self.min_pts):\n",
    "                # Initiate label for current data\n",
    "                if (neighbor_labels):\n",
    "                    self.labels[i] = min(neighbor_labels)\n",
    "                    # Change density reachable data's label to match this data's label\n",
    "                    self.__replace_label(neighbor_labels, self.labels[i])\n",
    "                else:\n",
    "                    self.labels[i] = self.__generate_new_label()\n",
    "                # Change neighbor's label\n",
    "                for j in range(self.__n_elmt):\n",
    "                    if (i != j):\n",
    "                        if (self.distance_matrix[i, j] <= self.eps):\n",
    "                            self.labels[j] = self.labels[i]\n",
    "\n",
    "    def fit_predict(self, X: DataFrame):\n",
    "        self.fit (X)\n",
    "        return self.labels\n",
    "\n",
    "    def __createDistanceMatrix(self, X: DataFrame):\n",
    "        self.distance_matrix = np.ndarray(shape=(self.__n_elmt, self.__n_elmt))\n",
    "        self.cluster_tree = [n for n in range(self.__n_elmt)]\n",
    "        for index, row in X.iterrows():\n",
    "            for index_2, row_2 in X.iterrows():\n",
    "                if (index_2 == index):\n",
    "                    distance = float('inf')\n",
    "                else:\n",
    "                    if (self.distance == \"manhattan\"):\n",
    "                        distance = math.fabs(row_2.subtract(row, fill_value=0).abs().sum())\n",
    "                    elif (self.distance == \"euclidean\"):\n",
    "                        distance = (row_2.subtract(row, fill_value=0).pow(2).sum()) ** 0.5\n",
    "                self.distance_matrix.itemset((index, index_2), distance)\n",
    "\n",
    "    def __generate_new_label(self):\n",
    "        label = 0\n",
    "        while (label in self.labels):\n",
    "            label += 1\n",
    "        return label\n",
    "\n",
    "    def __replace_label(self, old_labels, new_label):\n",
    "        for i in range (self.__n_elmt):\n",
    "            if (self.labels[i] in old_labels):\n",
    "                self.labels[i] = new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hasil Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from clustering import kmeans as kmean\n",
    "from clustering import kmedoids as kmed\n",
    "from clustering import agglomerative as agglo\n",
    "from clustering import dbscan as dbsc\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_data = pd.DataFrame(datasets.load_iris().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1]\n",
      "Centroid/Means: [[5.0059999999999993, 3.4180000000000006, 1.464, 0.24399999999999991], [5.8836065573770497, 2.7409836065573772, 4.3885245901639349, 1.4344262295081966], [6.8538461538461526, 3.0769230769230766, 5.7153846153846146, 2.0538461538461532]]\n",
      "Prediction: [2, 0]\n"
     ]
    }
   ],
   "source": [
    "# K-Means\n",
    "# kmeans = kmean.KMeans(num_clusters=3, init='random')\n",
    "# change init if needed\n",
    "kmeans = kmean.KMeans(num_clusters=3, init=[[5.1, 3.5, 1.4, 0.2], [5.0, 2.0, 3.5, 1.0], [5.9, 3.0, 5.1, 1.8]])\n",
    "test_data = np.array([[5.8,  3.1 ,  6.1,  0.4], [0.8,  1.1 ,  2.1,  0.1]])\n",
    "kmeans.fit(iris_data)\n",
    "print('Labels:', kmeans.labels)\n",
    "print('Centroid/Means:', kmeans.means)\n",
    "\n",
    "prediction = kmeans.predict(kmean.ndarray_to_dataframe(test_data))\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Medoids: [94, 7, 147]\n",
      "Prediction: [2, 1]\n"
     ]
    }
   ],
   "source": [
    "# K-Medoids\n",
    "kmedoids = kmed.KMedoids(num_clusters=3, init='random')\n",
    "# change swap_medoid if needed, default is optimized\n",
    "# kmedoids = KMedoids(num_clusters=3, init='random', swap_medoid='random')\n",
    "\n",
    "test_data = np.array([[5.8,  3.1 ,  6.1,  0.4], [0.8,  1.1 ,  2.1,  0.1]])\n",
    "kmedoids.fit(iris_data)\n",
    "print('Labels:', kmedoids.labels)\n",
    "print('Medoids:', kmedoids.medoids)\n",
    "\n",
    "prediction = kmedoids.predict(kmean.ndarray_to_dataframe(test_data))\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 148 is out of bounds for axis 0 with size 147",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-96f2bebfd9ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Agglomerative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magglomerative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magglo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgglomerative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"single\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cluster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"manhattan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0magglomerative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magglomerative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Labels:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\KULIAH IF\\Tingkat 4 Pilihan\\IF4071 - Pembelajaran Mesin\\Tugas\\tugas besar\\01-Clustering\\rolearnd\\rolearnd\\clustering\\agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Generate new distance matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mcombined_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__generateNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__removeNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mergeNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\KULIAH IF\\Tingkat 4 Pilihan\\IF4071 - Pembelajaran Mesin\\Tugas\\tugas besar\\01-Clustering\\rolearnd\\rolearnd\\clustering\\agglomerative.py\u001b[0m in \u001b[0;36m__generateNode\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__generateNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"single\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[0mrow_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mrow_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 148 is out of bounds for axis 0 with size 147"
     ]
    }
   ],
   "source": [
    "# Agglomerative\n",
    "agglomerative = agglo.Agglomerative(link=\"single\", n_cluster=3, distance=\"manhattan\")\n",
    "agglomerative.fit(iris_data)\n",
    "labels = agglomerative.predict()\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# DBSCAN\n",
    "dbscan = dbsc.DBSCAN(eps=0.5, min_pts=4, distance=\"euclidean\")\n",
    "labels = dbscan.fit_predict(iris_data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pembagian Tugas\n",
    "\n",
    "```\n",
    "13515089 : Agglomerative\n",
    "13515099 : DBSCAN\n",
    "13515107 : K-Means, K-Medoids\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
